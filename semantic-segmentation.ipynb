{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up\n",
    "\n",
    "### Import dependencies\n",
    "\n",
    "Run the below cell to import Python dependencies needed for displaying the results in this notebook\n",
    "(tip: select the cell and use **Ctrl+enter** to run the cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import sys                                     \n",
    "from pathlib import Path\n",
    "sys.path.insert(0, str(Path().resolve().parent.parent))\n",
    "from demoTools.demoutils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Intel® Distribution of OpenVINO™ toolkit\n",
    "\n",
    "First, let's try running inference on a single image to see how the Intel® Distribution of OpenVINO™ toolkit works.\n",
    "We will be using Intel® Distribution of OpenVINO™ toolkit Inference Engine (IE) to locate vehicles on the road.\n",
    "There are five steps involved in this task:\n",
    "\n",
    "1. Create an Intermediate Representation (IR) Model using the Model Optimizer by Intel\n",
    "2. Choose a device and create IEPlugin for the device\n",
    "3. Read the IRModel using IENetwork\n",
    "4. Load the IENetwork into the Plugin\n",
    "5. Run inference.\n",
    "\n",
    "### Creating IR Model\n",
    "\n",
    "The Model Optimizer creates Intermediate Representation (IR) models that are optimized for different end-point target devices.\n",
    "These models can be created from existing DNN models from popular frameworks (e.g. Caffe*, TF) using the Model Optimizer. \n",
    "\n",
    "The Intel® Distribution of OpenVINO™ toolkit includes a utility script `model_downloader.py` that you can use to download some common modes. Run the following cell to see the models available through `model_downloader.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "densenet-121\r\n",
      "densenet-161\r\n",
      "densenet-169\r\n",
      "densenet-201\r\n",
      "squeezenet1.0\r\n",
      "squeezenet1.1\r\n",
      "mtcnn-p\r\n",
      "mtcnn-r\r\n",
      "mtcnn-o\r\n",
      "mobilenet-ssd\r\n",
      "vgg19\r\n",
      "vgg16\r\n",
      "ssd512\r\n",
      "ssd300\r\n",
      "inception-resnet-v2\r\n",
      "dilation\r\n",
      "googlenet-v1\r\n",
      "googlenet-v2\r\n",
      "googlenet-v4\r\n",
      "alexnet\r\n",
      "ssd_mobilenet_v2_coco\r\n",
      "resnet-50\r\n",
      "resnet-101\r\n",
      "resnet-152\r\n",
      "googlenet-v3\r\n",
      "se-inception\r\n",
      "se-resnet-101\r\n",
      "se-resnet-152\r\n",
      "se-resnet-50\r\n",
      "se-resnext-50\r\n",
      "se-resnext-101\r\n",
      "Sphereface\r\n",
      "license-plate-recognition-barrier-0007\r\n",
      "mobilenet-v1-1.0-224\r\n",
      "mobilenet-v2\r\n",
      "faster_rcnn_inception_v2_coco\r\n",
      "deeplabv3\r\n",
      "ctpn\r\n",
      "ssd_mobilenet_v1_coco\r\n",
      "faster_rcnn_resnet101_coco\r\n",
      "mobilenet-v2-1.4-224\r\n",
      "age-gender-recognition-retail-0013\r\n",
      "age-gender-recognition-retail-0013-fp16\r\n",
      "emotions-recognition-retail-0003\r\n",
      "emotions-recognition-retail-0003-fp16\r\n",
      "face-detection-adas-0001\r\n",
      "face-detection-adas-0001-fp16\r\n",
      "face-detection-retail-0004\r\n",
      "face-detection-retail-0004-fp16\r\n",
      "face-person-detection-retail-0002\r\n",
      "face-person-detection-retail-0002-fp16\r\n",
      "face-reidentification-retail-0095\r\n",
      "face-reidentification-retail-0095-fp16\r\n",
      "facial-landmarks-35-adas-0002\r\n",
      "facial-landmarks-35-adas-0002-fp16\r\n",
      "head-pose-estimation-adas-0001\r\n",
      "head-pose-estimation-adas-0001-fp16\r\n",
      "human-pose-estimation-0001\r\n",
      "human-pose-estimation-0001-fp16\r\n",
      "landmarks-regression-retail-0009\r\n",
      "landmarks-regression-retail-0009-fp16\r\n",
      "license-plate-recognition-barrier-0001\r\n",
      "license-plate-recognition-barrier-0001-fp16\r\n",
      "pedestrian-and-vehicle-detector-adas-0001\r\n",
      "pedestrian-and-vehicle-detector-adas-0001-fp16\r\n",
      "pedestrian-detection-adas-0002\r\n",
      "pedestrian-detection-adas-0002-fp16\r\n",
      "person-attributes-recognition-crossroad-0230\r\n",
      "person-attributes-recognition-crossroad-0230-fp16\r\n",
      "person-detection-action-recognition-0005\r\n",
      "person-detection-action-recognition-0005-fp16\r\n",
      "person-detection-retail-0002\r\n",
      "person-detection-retail-0002-fp16\r\n",
      "person-detection-retail-0013\r\n",
      "person-detection-retail-0013-fp16\r\n",
      "person-reidentification-retail-0031\r\n",
      "person-reidentification-retail-0031-fp16\r\n",
      "person-reidentification-retail-0076\r\n",
      "person-reidentification-retail-0076-fp16\r\n",
      "person-reidentification-retail-0079\r\n",
      "person-reidentification-retail-0079-fp16\r\n",
      "person-vehicle-bike-detection-crossroad-0078\r\n",
      "person-vehicle-bike-detection-crossroad-0078-fp16\r\n",
      "road-segmentation-adas-0001\r\n",
      "road-segmentation-adas-0001-fp16\r\n",
      "semantic-segmentation-adas-0001\r\n",
      "semantic-segmentation-adas-0001-fp16\r\n",
      "single-image-super-resolution-1033\r\n",
      "single-image-super-resolution-1033-fp16\r\n",
      "text-detection-0002\r\n",
      "text-detection-0002-fp16\r\n",
      "vehicle-attributes-recognition-barrier-0039\r\n",
      "vehicle-attributes-recognition-barrier-0039-fp16\r\n",
      "vehicle-detection-adas-0002\r\n",
      "vehicle-detection-adas-0002-fp16\r\n",
      "vehicle-license-plate-detection-barrier-0106\r\n",
      "vehicle-license-plate-detection-barrier-0106-fp16\r\n",
      "face-detection-adas-binary-0001\r\n",
      "single-image-super-resolution-1032\r\n",
      "single-image-super-resolution-1032-fp16\r\n",
      "action-recognition-0001-encoder\r\n",
      "action-recognition-0001-encoder-fp16\r\n",
      "instance-segmentation-security-0049\r\n",
      "instance-segmentation-security-0049-fp16\r\n",
      "vehicle-detection-adas-binary-0001\r\n",
      "driver-action-recognition-adas-0002-decoder\r\n",
      "driver-action-recognition-adas-0002-decoder-fp16\r\n",
      "pedestrian-detection-adas-binary-0001\r\n",
      "person-detection-action-recognition-teacher-0002\r\n",
      "person-detection-action-recognition-teacher-0002-fp16\r\n",
      "instance-segmentation-security-0033\r\n",
      "instance-segmentation-security-0033-fp16\r\n",
      "action-recognition-0001-decoder\r\n",
      "action-recognition-0001-decoder-fp16\r\n",
      "text-recognition-0012\r\n",
      "text-recognition-0012-fp16\r\n",
      "driver-action-recognition-adas-0002-encoder\r\n",
      "driver-action-recognition-adas-0002-encoder-fp16\r\n",
      "gaze-estimation-adas-0002\r\n",
      "gaze-estimation-adas-0002-fp16\r\n",
      "resnet50-binary-0001\r\n"
     ]
    }
   ],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --print_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** the '!' is a special Jupyter Notebook command that allows you to run shell commands as if you are in a commannd line. So the above command will work straight out of the box on in a terminal (with '!' removed).\n",
    "\n",
    "In this demo, we will be using the **dilation** model. This model can be downloaded with the following command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "###############|| Downloading topologies ||###############\n",
      "\n",
      "========= Downloading raw_models/semantic_segmentation/dilation/cityscapes/caffe/dilation.prototxt\n",
      "... 100%, 9 KB, 36300 KB/s, 0 seconds passed\n",
      "\n",
      "========= Downloading raw_models/semantic_segmentation/dilation/cityscapes/caffe/dilation.caffemodel\n",
      "... 100%, 524898 KB, 28131 KB/s, 18 seconds passed\n",
      "\n",
      "\n",
      "###############|| Post processing ||###############\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!/opt/intel/openvino/deployment_tools/tools/model_downloader/downloader.py --name dilation -o raw_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input arguments are as follows:\n",
    "* --name : name of the model you want to download. It should be one of the models listed in the previous cell\n",
    "* -o : output directory. If this directory does not exist, it will be created for you.\n",
    "\n",
    "There are more arguments to this script and you can get the full list using the `-h` option.\n",
    "\n",
    "\n",
    "With the `-o` option set as above, this command downloads the model in the directory `raw_models`, with the `.caffemodel` located at `raw_models/semantic_segmentation/dilation/cityscapes/caffe/dilation.caffemodel`\n",
    "\n",
    "Now, let's convert this to the optimized model using the model optimizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating job file\n",
    "\n",
    "All the code up to this point has been run within the Jupyter Notebook instance running on a development node based on an Intel® Xeon® Scalable processor, where the Notebook is allocated a single core. \n",
    "To run inference and convert model we need more compute power and disk space.\n",
    "We will run the workload on edge compute nodes represented in the IoT DevCloud. We will send work to the edge compute nodes by submitting the corresponding non-interactive jobs into a queue. For each job, we will specify the type of the edge compute server that must be allocated for the job.\n",
    "\n",
    "The job file is written in Bash, and will be executed directly on the edge compute node.\n",
    "For this example, we have written the job files for you in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing convert_model.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile convert_model.sh\n",
    "\n",
    "cd $PBS_O_WORKDIR\n",
    "python3 /opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--framework caffe \\\n",
    "--input_shape [1,3,1396,1396] \\\n",
    "--output prob \\\n",
    "--input_model raw_models/semantic_segmentation/dilation/cityscapes/caffe/dilation.caffemodel \\\n",
    "--input_proto raw_models/semantic_segmentation/dilation/cityscapes/caffe/dilation.prototxt \\\n",
    "--data_type FP32 \\\n",
    "-o models/dilation/FP32 \\\n",
    "--mean_values [72.39,82.91,73.16] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand how jobs are submitted into the queue\n",
    "\n",
    "Now that we have the job script, we can submit the jobs to edge compute nodes. In the IoT DevCloud, you can do this using the `qsub` command.\n",
    "We can submit object_detection_job to several different types of edge compute nodes simultaneously or just one node at a time.\n",
    "\n",
    "There are three options of `qsub` command that we use for this:\n",
    "- `-l` : this option lets us select the number and the type of nodes using `nodes={node_count}:{property}`. \n",
    "- `-F` : this option lets us send arguments to the bash script. \n",
    "- `-N` : this option lets use name the job so that it is easier to distinguish between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submitting to an edge compute node with an Intel® CPU\n",
    "In the cell below, we submit a job to an <a \n",
    "    href=\"https://software.intel.com/en-us/iot/hardware/iei-tank-dev-kit-core\">IEI \n",
    "    Tank* 870-Q170</a> edge node with an <a \n",
    "    href=\"https://ark.intel.com/products/88186/Intel-Core-i5-6500TE-Processor-6M-Cache-up-to-3-30-GHz-\">Intel® Core™ i5-6500TE processor</a>.\n",
    "    The output of cell below is job id that is used for checking status of the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['34352.c003']\n"
     ]
    }
   ],
   "source": [
    "job_id = !qsub convert_model.sh -l nodes=1:tank-870:i5-6500te\n",
    "print(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check if the jobs are done\n",
    "\n",
    "To check on the jobs that were submitted, use the `qstat` command.\n",
    "\n",
    "We have created a custom Jupyter widget  to get live qstat update.\n",
    "Run the following cell to bring it up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0c164aca38e4222b2600fd0443efd69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid gray', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "baa7618a9b144505aa1cbd793aa69d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Stop', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the jobs you have submitted (referenced by `Job ID` that gets displayed right after you submit the job.\n",
    "There should also be an extra job in the queue \"jupyterhub\": this job runs your current Jupyter Notebook session.\n",
    "\n",
    "The 'S' column shows the current status. \n",
    "- If it is in Q state, it is in the queue waiting for available resources. \n",
    "- If it is in R state, it is running. \n",
    "- If the job is no longer listed, it means it is completed.\n",
    "\n",
    "**Note**: Time spent in the queue depends on the number of users accessing the edge nodes. Once these jobs begin to run, they should take from 1 to 5 minutes to complete. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit Inference\n",
    "\n",
    "After model convertion is finished we can submit job for inference model. Following cells create job script for inference, submit job to the server, displays statuses of current jobs and the final one displays original image and result of inference. You can use shift+enter hot key in order to start cell one by one faster. Please, note, that inference should be finished before running last cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing semantic_segmentation.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile semantic_segmentation.sh\n",
    "\n",
    "cd $PBS_O_WORKDIR\n",
    "python3 aist2019-openvino-semseg/semseg_inference.py \\\n",
    "--model=models/dilation/FP32/dilation.xml            \\\n",
    "--weights=models/dilation/FP32/dilation.bin          \\\n",
    "--input=data                                         \\\n",
    "--batch_size=1                                       \\\n",
    "--device=\"CPU\"                                       \\\n",
    "--color_map=aist2019-openvino-semseg/color_map.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34354.c003\n"
     ]
    }
   ],
   "source": [
    "job_id_core = !qsub semantic_segmentation.sh -l nodes=1:tank-870:i5-6500te -N sem_segm\n",
    "print(job_id_core[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6892cbd48c83483c84b8dc93d3e8cda8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output(layout=Layout(border='1px solid gray', width='100%'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b96b315474ae481a9c16692c498fc6e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Stop', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "liveQstat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "def show_img(name):\n",
    "    fig = plt.figure(dpi=300)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(cv2.cvtColor(cv2.imread(name), cv2.COLOR_BGR2RGB), interpolation='none')\n",
    "    \n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "    \n",
    "show_img('data/test.jpg')\n",
    "show_img('aist2019-openvino-semseg/out_segmentation_0.bmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu)",
   "language": "python",
   "name": "c003-python_3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
